{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster import hierarchy\n",
    "import gensim\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#import scipy.cluster.hierarchy as hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "#posts = pd.read_csv(r\"C:\\...\\2022-07-28-00-31-22-SGT-search-csv-export_lonely_loneliness_2020.csv\")\n",
    "posts = pd.read_csv(r\"C:\\...\\2022-07-28-00-32-18-SGT-search-csv-export_lonely_loneliness_2021.csv\")\n",
    "#posts = pd.read_csv(r\"C:\\...\\2022-07-28-00-34-46-SGT-search-csv-export_lonely_loneliness_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['Page Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "def get_tokens(x):\n",
    "    tokens = word_tokenize(str(x))\n",
    "    return tokens\n",
    "\n",
    "def no_stops(x):\n",
    "    #en_stops = set(stopwords.words('taglish'))\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    no_stops = ''\n",
    "    for word in get_tokens(x):\n",
    "        if word not in en_stops:\n",
    "            if len(word) > 3:\n",
    "                no_stops += word + ' '\n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing\n",
    "\n",
    "clean_posts = pd.DataFrame(posts['Page Description'].apply(no_stops))\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('[^\\w\\s]',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('[0-9]',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.lower()\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('facebook',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('philippine',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('philippines',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('www',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('post',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('photo',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('https',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('next',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('login',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('tinyurl',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('Singapore',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('singapore',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('official',' ')\n",
    "clean_posts['Page Description'] = clean_posts['Page Description'].str.replace('page',' ')\n",
    "clean_posts = pd.DataFrame(clean_posts['Page Description'].apply(no_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = TfidfVectorizer(stop_words=\"english\")\n",
    "vec1.fit(clean_posts['Page Description'].values)\n",
    "features1 = vec1.transform(clean_posts['Page Description'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "\n",
    "def getscore(x, y):\n",
    "    random_state = 2002\n",
    "    km = MiniBatchKMeans(n_clusters=x, random_state = random_state)\n",
    "    km.fit(y)\n",
    "    return 'Score for ' + str(x) + ' clusters: ' + str(silhouette_score(km.fit_transform(y), km.labels_))\n",
    "\n",
    "def getscores(x):\n",
    "    response = []\n",
    "    for i in range(3,30):\n",
    "        response.append(getscore(i, x))\n",
    "    return pd.DataFrame(response, columns=['outcomes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getscores(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2002\n",
    "cls1 = MiniBatchKMeans(n_clusters=9, random_state=random_state)\n",
    "cls1.fit(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the features to 2D\n",
    "pca1 = PCA(n_components=2, random_state=random_state)\n",
    "reduced_features1 = pca1.fit_transform(features1.toarray())\n",
    "\n",
    "# reduce the cluster centers to 2D\n",
    "reduced_cluster_centers1 = pca1.transform(cls1.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_features1[:,0], reduced_features1[:,1], c=cls1.predict(features1))\n",
    "plt.scatter(reduced_cluster_centers1[:, 0], reduced_cluster_centers1[:,1], marker='x', s=150, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position1 = pd.DataFrame(reduced_features1, index=clean_posts.index)\n",
    "position1.columns = ['x_position', 'y_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = pd.DataFrame(cls1.labels_, index=clean_posts.index)\n",
    "labels1.columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts_labeled = clean_posts.join(labels1).join(position1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clouds(clusters, corpus):\n",
    "    for x in range(clusters):\n",
    "        topic = corpus['Page Description'][corpus.label == x]\n",
    "        topics = ''\n",
    "        for i in topic.values:\n",
    "            topics += i\n",
    "        #topics\n",
    "        cloud = WordCloud(background_color='white', collocations=False).generate(topics)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.imshow(cloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        print(\"Topic #\" + str(x))\n",
    "        plt.show()\n",
    "        #plt.savefig(f'wordcloud.png',\n",
    "            #dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change based on number of clusters\n",
    "\n",
    "visualize_clouds(9, clean_posts_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(clusters, corpus):\n",
    "    for x in range(clusters):\n",
    "        topic = corpus['Page Description'][corpus.label == x]\n",
    "        topics = ''\n",
    "        for i in topic.values:\n",
    "            topics += i\n",
    "\n",
    "        #df=pd.DataFrame(r1,columns=['text'])\n",
    "        #df.text.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "        print(\"Topic #\" + str(x))\n",
    "        df = pd.DataFrame([topics], columns=['text'])\n",
    "        counts = df.text.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "        print(counts)\n",
    "        pd.DataFrame(counts).to_csv('counts' + str(x) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change based on number of clusters\n",
    "\n",
    "count_words(9, clean_posts_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts_labeled[clean_posts_labeled['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
